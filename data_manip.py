# -*- coding: utf-8 -*-
"""SalesForecast.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IaEYS0nd-hdzKA4cwbk8LV64rt1UCNiO
"""

import pandas as pd
import numpy as np
from datetime import timedelta
from sqlalchemy import create_engine
import psycopg2

engine = create_engine('postgresql://+id:pw@db link/dbname')

# df =  pd.read_sql('''SELECT date_order::date, date_delivered, default_code, product_uom_qty, qty_delivered from 
#             sale_quotation_line where state = 'process' and subcateg_trends_id IN ( 14, 13, 12, 11 ) 
#             and date_order :: DATE <= (now() + interval '15 hour')::date - 1 order by date_order::date''', engine)
# df

df =  pd.read_sql('''SELECT 
a.date_order:: DATE, 
a.state,
b.date_process,
a.date_delivered, 
a.default_code, 
a.product_uom_qty, 
a.qty_delivered 
FROM sale_quotation_line a
join sale_quotation b on b.id = a.order_id
WHERE
a.state != 'cancel'
and 
b.date_process is not null
AND a.date_order :: DATE > CURRENT_DATE - 60
and subcateg_trends_id IN ( 14, 13, 12, 11 )
ORDER BY a.date_order:: DATE''', engine)
df
# and a.default_code = 'E1039M'

# #SO outstanding dengan Cutoff data

# df =  pd.read_sql('''select 
# a.date_order::date,
# b.default_code,
# b.name,
# a.product_uom_qty * e.factor as order_qty,
# a.qty_delivered * e.factor as deliver_qty,
# (a.product_uom_qty * e.factor) - (a.qty_delivered * e.factor) as outstanding_qty,
# a.state,
# case when a.state in ('draft','open','process') then 0 else 1 end
# from 
# sale_quotation_line a 
# join product_product b on a.product_id = b.id
# join sale_quotation c on c.id = a.order_id
# join product_template d on d.id = b.product_tmpl_id
# left join product_uom e on d.production_uom_conversion_id = e.id
# where 
# a.date_order::date >= '2022-01-01' and a.date_order::date <= '2022-01-31' 
# and c.company_id = 1
# and c.state not in ('cancel','cart')
# and
# (select
# sum(ax.qty_delivered)
# from sale_quotation_line ax
# join stock_picking sp on ax.id = sp.so_id
# group by sp.so_id)
# order by a.date_order::date desc, a.order_id desc, outstanding_qty desc''', engine)
# print(df.head(5))

#

df['date_order'] = pd.to_datetime(df['date_order'])
df['date_delivered'] = pd.to_datetime(df['date_delivered'])
df['date_process'] = pd.to_datetime(df['date_process'])
print(df.dtypes)



df['date_process'] = pd.to_datetime(df['date_process'])
df['date_delivered'] = pd.to_datetime(df['date_delivered'])

grouped_so = df.groupby(['date_process', 'default_code','state'], as_index=False)['product_uom_qty'].sum()
grouped_do = df.groupby(['date_delivered', 'default_code','state'], as_index=False)['qty_delivered'].sum()

grouped_so.tail(2)

grouped_do.tail(2)

full_dates_so = pd.DataFrame(pd.date_range(start=df['date_process'].min(), end=(pd.to_datetime("now").date()-timedelta(days=1))), columns=['date_process'])
full_dates_do = pd.DataFrame(pd.date_range(start=df['date_delivered'].min(), end=(pd.to_datetime("now").date()-timedelta(days=1))), columns=['date_delivered'])
print(full_dates_so.head(5))
print(full_dates_do.head(5))



# df_pivot_deliver = pd.merge(full_dates_do, status, how="left", on=["date_delivered"])
# df_pivot_deliver.fillna('process', inplace=True)
# df_pivot_deliver

df_pivot_order= pd.pivot_table(grouped_so, values='product_uom_qty', index=['date_process'],
                    columns=['default_code'],fill_value=0)
df_pivot_order.reset_index(inplace=True)
df_pivot_order.tail(5)

df_pivot_order = pd.merge(full_dates_so, df_pivot_order, how="left", on=["date_process"])
df_pivot_order.fillna(0, inplace=True)
df_pivot_order.tail(5)

df_order =  pd.melt(df_pivot_order, 
            id_vars=['date_process'], 
            value_vars=list(df_pivot_order.columns[1:]), # list of days of the week
            var_name='default_code',
            value_name='order_qty')
df_order

grouped_order = df_order.groupby(['date_process', 'default_code'], as_index=False)['order_qty'].sum()
print(grouped_order.head(5))

grouped_order.columns = ['date', 'default_code', 'order_qty']
# grouped_order.columns = ['date', 'default_code', 'order_qty']
print(grouped_order.tail(5))

df_pivot_deliver= pd.pivot_table(grouped_do, values='qty_delivered', index=['date_delivered'],
                    columns=['default_code'],fill_value=0)
df_pivot_deliver.reset_index(inplace=True)
print(df_pivot_deliver.head(5))

df_pivot_deliver = pd.merge(full_dates_do, df_pivot_deliver, how="left", on=["date_delivered"])
df_pivot_deliver.fillna(0, inplace=True)
print(df_pivot_deliver.head(5))

df_deliver =  pd.melt(df_pivot_deliver, 
            id_vars=['date_delivered'],
            value_vars=list(df_pivot_deliver.columns[1:]), # list of days of the week
            var_name='default_code', 
            value_name='qty_delivered')
df_deliver

grouped_deliver = df_deliver.groupby(['date_delivered', 'default_code'], as_index=False)['qty_delivered'].sum()
grouped_deliver.columns = ['date', 'default_code', 'qty_delivered']
# grouped_deliver.columns = ['date_delivered', 'default_code', 'qty_delivered']
print(grouped_deliver.head(5))

df_merge = pd.merge(grouped_order, grouped_deliver, how="left", on=["default_code", "date"])
df_merge.fillna(0, inplace=True)
print(df_merge.head(5))
# df_merge = pd.merge(grouped_order, grouped_deliver, how="left", on=["default_code"])
print(df_merge.head(5))

df_merge.fillna(0, inplace=True)
df_merge['selisih'] = df_merge['order_qty'] - df_merge['qty_delivered']
df_merge.head(5)

print(df_merge.head(5))

df_merge_sum=df_merge.sort_values(['date']).reset_index(drop=True)
print(df_merge_sum.head(5))

"""cumsum = Cumulative sum (Grafik Jumlah Kumulatif) meningkatkan kemampuan untuk mendeteksi pergeseran kecil dengan memetakan statistik yang menggabungkan nilai data saat ini dan sebelumnya dari proses. Secara khusus, bagan CUSUM memplot jumlah kumulatif penyimpangan nilai sampel dari nilai target"""

df_merge_sum["progress_sum"]=df_merge_sum.groupby(['default_code'])['selisih'].cumsum(axis=0)
print(df_merge_sum.head(20))

df_merge_sum.to_csv("so_progress_all.csv")
df_merge_sum

print(df_merge_sum.dtypes)

"""#BAGIAN STOCK_SO"""

df_stock =  pd.read_sql('''SELECT
	fq.date_of_trans as date,
	fq.code_prod as default_code,
	fq.name_prod,
	fq.subcateg_name,
	COALESCE(SUM(fq.st_in), 0) as st_in,
	COALESCE(SUM(fq.st_out), 0) as st_out,
	COALESCE(SUM(fq.st_last),0) as st_last
FROM show_stock_per_date_by_trend('{73,74,34,322,342,295,389,296,40,372,327,297,328,394}','{14,13,12,11}',True,30) fq 
GROUP BY
	fq.date_of_trans,
	fq.product,
	fq.code_prod,
	fq.name_prod,
	fq.subcateg_name
ORDER BY fq.date_of_trans''', engine)

df_stock['date'] = pd.to_datetime(df_stock['date'])

df_merge_stock_so = pd.merge(df_stock, df_merge_sum, how="left", on=["date", "default_code"])
df_merge_stock_so.fillna(0, inplace=True)

df_merge_stock_so['stock-so'] = df_merge_stock_so['st_last'] - df_merge_stock_so['progress_sum']
df_merge_stock_so.to_csv('data_stock_so.csv')

df_merge_stock_so

"""# Google Drive"""

#from google.colab import files
  
#uploaded = files.upload()

"""Upload Credential code

Cara ini hanya valid untuk 3 jam (Token Expired)
"""

#import json
#import requests

# """headers = {"Authorization": "Bearer ya29.a0ARrdaM9iSD5jpHChOG4zhVcOdxl51O73V54Y-vR0hv6EjrPsbgGILHOkrGyUDh0kwYponqeSaGBSpc4axXf0tYglP_wHlPhdJxaKyPPJ4JwPRyQdaIeH8ydrV4p8MIuiIS9tu0vkvRsKmWky7nMQ8i9Y_LXR"}
# para = {
#     "name": "coba_test2.csv",
# }
# files = {
#     'data': ('metadata', json.dumps(para), 'application/json; charset=UTF-8'),
#     'file': open("./data_stock_so.csv", "rb")
# }
# r = requests.post(
#    "https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart",
#   headers=headers,
#  files=files
# )
# print(r.text)"""



# '''import os
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive'''

# """gauth = GoogleAuth()

# """gauth.LocalWebserverAuth()

# """drive = GoogleDrive(gauth)

# '''f = drive.CreateFile()
# f.SetContentFile('data_stock_so.csv')
# f.Upload()

"""#Gdrive Done Uploaded """



"""# Pake FTP"""

# import ftplib

# FTP_HOST = "Win"
# FTP_USER = "username"
# FTP_PASS = "password"

# ftp = ftplib.FTP(FTP_HOST, FTP_USER, FTP_PASS)
# # force UTF-8 encoding
# ftp.encoding = "utf-8"



# # local file name you want to upload
# coba = open('read.txt', 'w')
# coba.write('Welcome to Geeks for Geeks')
# coba.close()

# filename = "read.txt"
# with open(filename, "rb") as file:
#     # use FTP's STOR command to upload the file
#     ftp.storbinary(f"STOR {filename}", file)

# ftp.dir()







# from google.colab import files
  
# uploaded = files.upload()

# from __future__ import print_function
# import os.path
# from google.auth.transport.requests import Request
# from google.oauth2.credentials import Credentials
# from google_auth_oauthlib.flow import InstalledAppFlow
# from googleapiclient.discovery import build
# from googleapiclient.errors import HttpError
# from oauth2client.service_account import ServiceAccountCredentials
# from apiclient.discovery import build
# from apiclient.http import MediaFileUpload

# # If modifying these scopes, delete the file token.json.
# SCOPES = ['https://www.googleapis.com/auth/drive',
#     'https://www.googleapis.com/auth/drive.file']

# def main():
#     """Shows basic usage of the Drive v3 API.
#     Prints the names and ids of the first 10 files the user has access to.
#     """
#     creds = None
#     # The file token.json stores the user's access and refresh tokens, and is
#     # created automatically when the authorization flow completes for the first
#     # time.
#     print ('start')
#     if os.path.exists('token.json'):
#         creds = Credentials.from_authorized_user_file('token.json', SCOPES)
#     # If there are no (valid) credentials available, let the user log in.
#     print('creds=',creds)
#     if not creds or not creds.valid:
#         if creds and creds.expired and creds.refresh_token:
#             creds.refresh(Request())
#         else:
#             print('else',SCOPES)
#             flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
#             print ('flow=',flow)
#             creds = flow.run_local_server(port=8090)
#             print('res=',flow,creds)
#         # Save the credentials for the next run
#         with open('token.pickle', 'w') as token:
#             token.write(creds.to_json())
#     try:
#         print ('try')
#         service = build('drive', 'v3', credentials=creds)
#         print('service=',service)
#         # Call the Drive v3 API
#         results = service.files().list(
#             pageSize=10, fields="nextPageToken, files(id, name)").execute()
#         items = results.get('files', [])
#         if not items:
#             print('No files found.')
#             return
#         print('Files:')
#         for item in items:
#             print(u'{0} ({1})'.format(item['name'], item['id']))
#     except HttpError as error:
#         # TODO(developer) - Handle errors from drive API.
#         print(f'An error occurred: {error}')
        
# if __name__ == '__main__':
#     main()

